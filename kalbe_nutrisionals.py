# -*- coding: utf-8 -*-
"""KALBE Nutrisionals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U44VkosFHIHdzYAt2L-ymCkovPByeuS-
"""

pip install pmdarima

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.impute import KNNImputer
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import ExponentialSmoothing, Holt
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import pmdarima as pm
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_samples, silhouette_score
from itertools import permutations
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

import warnings
warnings.filterwarnings("ignore")

print('numpy version : ',np.__version__)
print('pandas version : ',pd.__version__)
print('seaborn version : ',sns.__version__)

from google.colab import drive
drive.mount('/content/drive', force_remount = True)

path = "/content/drive/MyDrive/Bootcamp Data Science Rakamin/flight.csv"
df_customer = pd.read_excel("/content/drive/MyDrive/Rakamin/PBI/customer.xlsx")
df_product = pd.read_excel("/content/drive/MyDrive/Rakamin/PBI/product.xlsx")
df_store = pd.read_excel("/content/drive/MyDrive/Rakamin/PBI/store.xlsx")
df_transaction = pd.read_excel("/content/drive/MyDrive/Rakamin/PBI/transaction.xlsx")

df_customer.sample(5)

df_product.sample(5)

df_store.sample(5)

df_transaction.sample(5)

# convert Date to datetime
df_transaction['Date'] = pd.to_datetime(df_transaction['Date'], format='%d/%m/%Y')

# fill missing values
df_customer.isna().sum()
df_customer.fillna(method='ffill', inplace=True)

#Merge data
df_merge = pd.merge(df_transaction, df_product, on='ProductID', how='left')
df_merge = pd.merge(df_merge, df_store, on='StoreID', how='left')
df_merge = pd.merge(df_merge, df_customer, on='CustomerID', how='left')
df_merge.head(5)

df_merge.info()

#Check null
df_merge.isna().sum()

# create Regression Model (Time Series)
df_reg = df_transaction.groupby('Date')['Qty'].sum().reset_index()
df_reg['Date'] = pd.to_datetime(df_reg['Date'], format='%d/%m/%Y')
df_reg.sort_values(by='Date', inplace=True)
df_reg.set_index('Date', inplace=True)

df_reg

decompose = seasonal_decompose(df_reg)

fig,ax = plt.subplots(3,1,figsize=(15,12))
decompose.trend.plot(ax=ax[0])
ax[0].set_title('Trend')
decompose.seasonal.plot(ax=ax[1])
ax[1].set_title('Seasonal')
decompose.resid.plot(ax=ax[2])
ax[2].set_title('Residual')

plt.tight_layout()
plt.show()

#Split Data Train & Test
print(df_reg.shape)
test_size = round(df_reg.shape[0] * 0.15)
train=df_reg.iloc[:-1*(test_size)]
test=df_reg.iloc[-1*(test_size):]
print(train.shape,test.shape)

plt.figure(figsize=(12,5))
sns.lineplot(data=train, x=train.index, y=train['Qty'])
sns.lineplot(data=test, x=test.index, y=test['Qty'])
plt.show()

#Data Stationary
from statsmodels.tsa.stattools import adfuller
def adf_test(dataset):
     dftest = adfuller(dataset, autolag = 'AIC')
     print("1. ADF : ",dftest[0])
     print("2. P-Value : ", dftest[1])
     print("3. Num Of Lags : ", dftest[2])
     print("4. Num Of Observations Used For ADF Regression:",      dftest[3])
     print("5. Critical Values :")
     for key, val in dftest[4].items():
         print("\t",key, ": ", val)
adf_test(df_reg)

#auto-fit ARIMA
auto_arima = pm.auto_arima(train, stepwise=False, seasonal=False)
auto_arima

from itertools import product
# Make a list for p, d, and q
p = range(0, 4)
d = range(0, 4)
q = range(0, 4)
# Use the product function from itertools to create a combination of p, d, and q
pdq = list(product(p, d, q))
print(pdq)

from statsmodels.tsa.arima.model import ARIMA

# Initialize list to store AIC values
aic_scores = []

# Perform a manual grid search to find the optimal p, d, q parameters
for parameters in pdq:
    # Make ARIMA Model
    model = ARIMA(df_reg, order=parameters)
    model_fit = model.fit()
    # Stores AIC values ​​in a list
    aic_scores.append({'param': parameters, 'aic': model_fit.aic})

# Select the parameter with the lowest AIC
best_aic = min(aic_scores, key=lambda x: x['aic'])
print("Best parameters based on AIC:", best_aic)

#Hyperparameter tuning
model_hyper = ARIMA(train, order=best_aic['param'])
model_fit_hyper = model_hyper.fit()

#Trial and error tuning
model_manual = ARIMA(train, order=(40,2,2))
model_fit_manual = model_manual.fit()

import matplotlib.pyplot as plt

manual_forecast = model_fit_manual.forecast(steps=len(test))
hyper_forecast = model_fit_hyper.forecast(steps=len(test))
auto_forecast = auto_arima.predict(steps=len(test))

plot_data = df_reg.iloc[-100:]

plot_data['Manual Forecast'] = [None] * (len(plot_data) - len(manual_forecast)) + list(manual_forecast)
plot_data['Hyper Forecast'] = [None] * (len(plot_data) - len(hyper_forecast)) + list(hyper_forecast)
plot_data['Auto Forecast'] = [None] * (len(plot_data) - len(auto_forecast)) + list(auto_forecast)

plt.figure(figsize=(50, 1))

plot_data.plot()
plt.show()

from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

manual_mae = mean_absolute_error(test, manual_forecast)
manual_mape = mean_absolute_percentage_error(test, manual_forecast)
manual_rmse = np.sqrt(mean_squared_error(test, manual_forecast))

print(f'MAE (Manual Forecast): {round(manual_mae, 4)}')
print(f'MAPE (Manual Forecast): {round(manual_mape, 4)}')
print(f'RMSE (Manual Forecast): {round(manual_rmse, 4)}')

from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

hyper_mae = mean_absolute_error(test, hyper_forecast)
hyper_mape = mean_absolute_percentage_error(test, hyper_forecast)
hyper_rmse = np.sqrt(mean_squared_error(test, hyper_forecast))

print(f'MAE (Auto ARIMA): {round(hyper_mae, 4)}')
print(f'MAPE (Auto ARIMA): {round(hyper_mape, 4)}')
print(f'RMSE (Auto ARIMA): {round(hyper_rmse, 4)}')

#Apply model to forecast data
model = ARIMA(df_reg, order=(40, 2, 2))
model_fit = model.fit()
forecast = model_fit.forecast(steps=31)

#Plot forecasting
plt.figure(figsize=(12,5))
plt.plot(df_reg)
plt.plot(forecast,color='orange')
plt.title('Quantity Sales Forecasting')
plt.show()

forecast.describe()

"""###CLUSTERING"""

# Creating new dataframe
df_cluster = df_merge.groupby(['CustomerID']).agg({'TransactionID':'count',
                                          'Qty':'sum',
                                          'TotalAmount':'sum'}).reset_index()

# Normalizing data
from sklearn.preprocessing import MinMaxScaler
new_data = MinMaxScaler().fit_transform(df_cluster)
df_nor = pd.DataFrame(new_data, columns=df_cluster.columns)
df_nor.describe()

df_nor = df_nor.drop(columns=['CustomerID'])
df_nor.columns

# Aggregate data
agg = {
    'TransactionID': 'count',
    'Qty': 'sum',
    'TotalAmount': 'sum'
}
cluster_df = df_merge.groupby('CustomerID').aggregate(agg).reset_index()
cluster_df.head()

# scale data into same range
scaler = StandardScaler()
scaled_df = scaler.fit_transform(cluster_df[['TransactionID', 'Qty', 'TotalAmount']])
scaled_df = pd.DataFrame(scaled_df, columns=['TransactionID', 'Qty', 'TotalAmount'])
scaled_df.head()

# finding optimal number of clusters
inertia = []
max_clusters = 11
for n_cluster in range(1, max_clusters):
    kmeans = KMeans(n_clusters=n_cluster, random_state=42, n_init=n_cluster)
    kmeans.fit(cluster_df.drop('CustomerID', axis=1))
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(10,8))
plt.plot(np.arange(1, max_clusters), inertia, marker='o')
plt.xlabel('Number of cluster')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.xticks(np.arange(1, max_clusters))
plt.show()

K = range(2,8)
fits=[]
score=[]

for k in K:
    model = KMeans(n_clusters = k, random_state = 0, n_init= 'auto').fit(df_nor)
    fits.append(model)
    score.append(silhouette_score(df_nor, model.labels_, metric='euclidean'))

sns.lineplot(x = K, y = score)

fits[2]

cluster_df['cluster']= fits[2].labels_
cluster_df

cluster_df.groupby(['cluster']).agg({
    'CustomerID' : 'count',
    'TransactionID': 'mean',
    'Qty':'mean',
    'TotalAmount':'mean'
})